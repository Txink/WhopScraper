#!/usr/bin/env python3
"""
å¯¼å‡ºé¡µé¢ DOMã€æˆªå›¾ã€ç»“æ„åˆ†æå’Œæ¶ˆæ¯æ•°æ®ï¼Œä¾›æœ¬åœ°åˆ†æå’Œè°ƒè¯•ã€‚

åŠŸèƒ½ï¼š
1. å¯¼å‡ºå®Œæ•´ HTML é¡µé¢å†…å®¹ï¼ˆdebug/page_*.htmlï¼‰
2. æˆªå–å…¨å±æˆªå›¾ï¼ˆdebug/page_*.pngï¼‰
3. åˆ†æé¡µé¢ç»“æ„ï¼ˆdebug/analysis_*.txtï¼‰
4. æå–å¹¶å¯¼å‡ºæ¶ˆæ¯æ•°æ®åˆ° data/origin_message.json
   - åŒ…å«å®Œæ•´çš„æ¶ˆæ¯ç»„ã€å¼•ç”¨ã€å†å²è®°å½•
   - è‡ªåŠ¨å»é‡ï¼ˆæŒ‰ domIDï¼‰
   - æŒ‰æ—¶é—´æ’åº
   - å¢é‡æ›´æ–°ï¼ˆä¸è¦†ç›–å·²æœ‰æ¶ˆæ¯ï¼‰
5. æ˜¾ç¤ºè¯¦ç»†çš„æ¶ˆæ¯ç»Ÿè®¡ä¿¡æ¯

è¿è¡Œ: python test/test_export_page_dom.py  æˆ–  python -m test.test_export_page_dom
"""
import asyncio
import json
import os
import sys
from datetime import datetime
from pathlib import Path

sys.path.insert(0, str(Path(__file__).resolve().parent.parent))

from config import Config
from scraper.browser import BrowserManager
from scraper.message_extractor import EnhancedMessageExtractor


async def export_page_dom():
    """å¯¼å‡ºé¡µé¢DOMå’Œæˆªå›¾ä¾›æœ¬åœ°åˆ†æ"""
    print("\n" + "=" * 60)
    print("å¯¼å‡ºé¡µé¢DOMå’Œæˆªå›¾")
    print("=" * 60 + "\n")

    # éªŒè¯é…ç½®
    if not Config.validate():
        print("âŒ é…ç½®éªŒè¯å¤±è´¥")
        return

    print("âœ… é…ç½®éªŒè¯é€šè¿‡\n")

    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir = "debug"
    os.makedirs(output_dir, exist_ok=True)

    # åˆ›å»ºæµè§ˆå™¨ç®¡ç†å™¨
    browser = BrowserManager(
        headless=False,  # ä½¿ç”¨éæ— å¤´æ¨¡å¼ä¾¿äºæŸ¥çœ‹
        slow_mo=Config.SLOW_MO,
        storage_state_path=Config.STORAGE_STATE_PATH
    )

    try:
        # å¯åŠ¨æµè§ˆå™¨
        print("ğŸš€ æ­£åœ¨å¯åŠ¨æµè§ˆå™¨...")
        page = await browser.start()
        print("âœ… æµè§ˆå™¨å·²å¯åŠ¨\n")

        # è·å–æ‰€æœ‰éœ€è¦ç›‘æ§çš„é¡µé¢é…ç½®
        page_configs = Config.get_all_pages()

        if not page_configs:
            print("âŒ æ²¡æœ‰é…ç½®ä»»ä½•ç›‘æ§é¡µé¢")
            return

        # æ£€æŸ¥ç™»å½•çŠ¶æ€
        first_url = page_configs[1][0]
        print("ğŸ” æ­£åœ¨æ£€æŸ¥ç™»å½•çŠ¶æ€...")
        if not await browser.is_logged_in(first_url):
            print("âš ï¸  éœ€è¦ç™»å½•...")
            success = await browser.login(
                Config.WHOP_EMAIL,
                Config.WHOP_PASSWORD,
                Config.LOGIN_URL
            )

            if not success:
                print("âŒ ç™»å½•å¤±è´¥ï¼Œè¯·æ£€æŸ¥å‡­æ®æ˜¯å¦æ­£ç¡®")
                return
            print("âœ… ç™»å½•æˆåŠŸ\n")
        else:
            print("âœ… å·²ç™»å½•\n")

        # å¯¼èˆªåˆ°é¡µé¢
        test_url, test_type, _ = page_configs[1]
        print(f"ğŸ“„ æ­£åœ¨è®¿é—®é¡µé¢: [{test_type.upper()}] {test_url}")

        if not await browser.navigate(test_url):
            print(f"âŒ æ— æ³•å¯¼èˆªåˆ°é¡µé¢: {test_url}")
            return

        print("âœ… é¡µé¢å¯¼èˆªæˆåŠŸ\n")

        # ç­‰å¾…é¡µé¢åˆå§‹åŠ è½½
        print("â³ ç­‰å¾…é¡µé¢åˆå§‹åŠ è½½...")
        await asyncio.sleep(3)

        # ç­‰å¾…ç”¨æˆ·ç¡®è®¤
        print("\n" + "=" * 60)
        print("âš ï¸  é‡è¦æç¤º")
        print("=" * 60)
        print("\næµè§ˆå™¨çª—å£å·²æ‰“å¼€ï¼Œè¯·åœ¨æµè§ˆå™¨ä¸­æ‰§è¡Œä»¥ä¸‹æ“ä½œï¼š")
        print("\n1. ğŸ“œ æ»šåŠ¨é¡µé¢åˆ°æœ€åº•éƒ¨ï¼ŒåŠ è½½æ‰€æœ‰å†å²æ¶ˆæ¯")
        print("2. â³ ç­‰å¾…æ‰€æœ‰æ¶ˆæ¯å®Œå…¨åŠ è½½")
        print("3. âœ… ç¡®è®¤é¡µé¢å†…å®¹å®Œæ•´")
        print("\nå®ŒæˆåæŒ‰ [å›è½¦] é”®ç»§ç»­å¯¼å‡º...\n")

        # ç­‰å¾…ç”¨æˆ·è¾“å…¥
        input()

        print("\nâœ… æ”¶åˆ°ç¡®è®¤ï¼Œå¼€å§‹å¯¼å‡º...\n")

        # ç”Ÿæˆæ—¶é—´æˆ³
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

        # 1. å¯¼å‡ºå®Œæ•´HTML
        html_file = f"{output_dir}/page_{timestamp}.html"
        print(f"ğŸ“ æ­£åœ¨å¯¼å‡ºHTMLåˆ°: {html_file}")
        html_content = await page.content()
        with open(html_file, 'w', encoding='utf-8') as f:
            f.write(html_content)
        print(f"âœ… HTMLå·²ä¿å­˜ ({len(html_content)} å­—ç¬¦)\n")

        # 2. æˆªå›¾
        screenshot_file = f"{output_dir}/page_{timestamp}.png"
        print(f"ğŸ“¸ æ­£åœ¨æˆªå›¾åˆ°: {screenshot_file}")
        await page.screenshot(path=screenshot_file, full_page=True)
        print(f"âœ… æˆªå›¾å·²ä¿å­˜\n")

        # 3. å¯¼å‡ºæ¶ˆæ¯ç»“æ„åˆ†æ
        analysis_file = f"{output_dir}/analysis_{timestamp}.txt"
        print(f"ğŸ” æ­£åœ¨åˆ†æé¡µé¢ç»“æ„...")

        # ä½¿ç”¨JavaScriptåˆ†æé¡µé¢ç»“æ„
        js_analysis = """
        () => {
            const analysis = {
                url: window.location.href,
                title: document.title,
                all_elements_count: document.querySelectorAll('*').length,

                // æŸ¥æ‰¾å¯èƒ½çš„æ¶ˆæ¯å®¹å™¨
                potential_message_containers: [],

                // æŸ¥æ‰¾å¯èƒ½çš„æ–‡æœ¬å†…å®¹
                text_elements: []
            };

            // å°è¯•å¤šç§å¯èƒ½çš„é€‰æ‹©å™¨
            const selectors = [
                '[class*="message"]',
                '[class*="Message"]',
                '[class*="post"]',
                '[class*="Post"]',
                '[class*="content"]',
                '[class*="Content"]',
                '[role="article"]',
                'article',
                '[data-message]',
                '[data-post]'
            ];

            for (const selector of selectors) {
                const elements = document.querySelectorAll(selector);
                if (elements.length > 0) {
                    const sample = elements[0];
                    analysis.potential_message_containers.push({
                        selector: selector,
                        count: elements.length,
                        sample_classes: sample.className,
                        sample_id: sample.id,
                        sample_attributes: Array.from(sample.attributes).map(a => `${a.name}="${a.value.substring(0, 50)}"`),
                        sample_text: sample.innerText.substring(0, 200),
                        sample_html: sample.outerHTML.substring(0, 500)
                    });
                }
            }

            // æŸ¥æ‰¾åŒ…å«ç‰¹å®šå…³é”®å­—çš„å…ƒç´ 
            const keywords = ['GILD', 'CALL', 'PUT', 'æ­¢æŸ', 'å‡º'];
            const walker = document.createTreeWalker(
                document.body,
                NodeFilter.SHOW_TEXT,
                null,
                false
            );

            let node;
            while (node = walker.nextNode()) {
                const text = node.textContent.trim();
                if (text.length > 10) {
                    for (const keyword of keywords) {
                        if (text.includes(keyword)) {
                            let element = node.parentElement;
                            let depth = 0;
                            const path = [];

                            while (element && depth < 5) {
                                path.push({
                                    tag: element.tagName,
                                    class: element.className,
                                    id: element.id
                                });
                                element = element.parentElement;
                                depth++;
                            }

                            analysis.text_elements.push({
                                text: text.substring(0, 100),
                                keyword: keyword,
                                path: path
                            });
                            break;
                        }
                    }
                }
            }

            return analysis;
        }
        """

        analysis_data = await page.evaluate(js_analysis)

        with open(analysis_file, 'w', encoding='utf-8') as f:
            f.write("=" * 60 + "\n")
            f.write("é¡µé¢ç»“æ„åˆ†æ\n")
            f.write("=" * 60 + "\n\n")
            f.write(f"URL: {analysis_data['url']}\n")
            f.write(f"æ ‡é¢˜: {analysis_data['title']}\n")
            f.write(f"æ€»å…ƒç´ æ•°: {analysis_data['all_elements_count']}\n\n")

            f.write("=" * 60 + "\n")
            f.write("å¯èƒ½çš„æ¶ˆæ¯å®¹å™¨é€‰æ‹©å™¨\n")
            f.write("=" * 60 + "\n\n")

            for i, container in enumerate(analysis_data['potential_message_containers'], 1):
                f.write(f"{i}. é€‰æ‹©å™¨: {container['selector']}\n")
                f.write(f"   æ•°é‡: {container['count']}\n")
                f.write(f"   ç±»å: {container['sample_classes']}\n")
                f.write(f"   ID: {container['sample_id']}\n")
                f.write(f"   å±æ€§:\n")
                for attr in container['sample_attributes']:
                    f.write(f"      {attr}\n")
                f.write(f"\n   ç¤ºä¾‹æ–‡æœ¬:\n   {container['sample_text']}\n")
                f.write(f"\n   ç¤ºä¾‹HTML:\n   {container['sample_html']}\n")
                f.write("\n" + "-" * 60 + "\n\n")

            f.write("\n" + "=" * 60 + "\n")
            f.write("åŒ…å«äº¤æ˜“å…³é”®å­—çš„å…ƒç´ \n")
            f.write("=" * 60 + "\n\n")

            for i, elem in enumerate(analysis_data['text_elements'][:20], 1):
                f.write(f"{i}. å…³é”®å­—: {elem['keyword']}\n")
                f.write(f"   æ–‡æœ¬: {elem['text']}\n")
                f.write(f"   è·¯å¾„:\n")
                for j, node in enumerate(elem['path']):
                    indent = "   " * (j + 2)
                    f.write(f"{indent}<{node['tag']} class='{node['class']}' id='{node['id']}'>\n")
                f.write("\n")

        print(f"âœ… åˆ†æå·²ä¿å­˜\n")

        # 4. æå–æ¶ˆæ¯å¹¶å¯¼å‡ºä¸ºJSON
        messages_file = "data/origin_message.json"
        print(f"ğŸ’¬ æ­£åœ¨æå–æ¶ˆæ¯...")
        
        # ä½¿ç”¨ EnhancedMessageExtractor æå–æ¶ˆæ¯
        extractor = EnhancedMessageExtractor(page)
        try:
            message_groups = await extractor.extract_message_groups()
            print(f"âœ… æˆåŠŸæå– {len(message_groups)} æ¡æ¶ˆæ¯\n")
            
            # è½¬æ¢ä¸ºå­—å…¸åˆ—è¡¨
            new_messages = [msg.to_simple_dict() for msg in message_groups]
            
            # æŒ‰æ—¶é—´æ’åºè¾…åŠ©å‡½æ•°
            def parse_timestamp(ts_str: str) -> datetime:
                """è§£ææ—¶é—´æˆ³å­—ç¬¦ä¸²ï¼ˆæ¶ˆæ¯å·²ç»é€šè¿‡ normalize_timestamp å¤„ç†è¿‡ï¼‰"""
                try:
                    # å°è¯•å¤šç§æ—¶é—´æ ¼å¼ï¼ˆä¼˜å…ˆåŒ¹é…æ ‡å‡†åŒ–åçš„æ ¼å¼ï¼‰
                    formats = [
                        "%Y-%m-%d %H:%M:%S.%f",  # 2026-02-03 20:44:55.010 (æ ‡å‡†åŒ–æ ¼å¼)
                        "%Y-%m-%d %H:%M:%S",     # 2026-02-03 20:44:55
                        "%b %d, %Y %I:%M %p",    # Jan 06, 2026 11:38 PM (æœªæ ‡å‡†åŒ–çš„åŸå§‹æ ¼å¼)
                    ]
                    for fmt in formats:
                        try:
                            return datetime.strptime(ts_str, fmt)
                        except ValueError:
                            continue
                    
                    # å¦‚æœæ ‡å‡†æ ¼å¼éƒ½å¤±è´¥ï¼Œå°è¯•ä½¿ç”¨ EnhancedMessageExtractor çš„æ ‡å‡†åŒ–å‡½æ•°
                    # ä½†è¿™ç§æƒ…å†µç†è®ºä¸Šä¸åº”è¯¥å‘ç”Ÿï¼Œå› ä¸ºæ¶ˆæ¯å·²ç»è¢«æ ‡å‡†åŒ–äº†
                    normalized = EnhancedMessageExtractor.normalize_timestamp(ts_str, 0)
                    if normalized != ts_str:
                        # æ ‡å‡†åŒ–æˆåŠŸï¼Œå°è¯•å†æ¬¡è§£æ
                        for fmt in formats:
                            try:
                                return datetime.strptime(normalized, fmt)
                            except ValueError:
                                continue
                    
                    # å¦‚æœéƒ½å¤±è´¥ï¼Œè¿”å›ä¸€ä¸ªé»˜è®¤å€¼
                    return datetime.min
                except Exception:
                    return datetime.min
            
            # è¯»å–ç°æœ‰æ¶ˆæ¯ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
            existing_messages = []
            existing_dom_ids = set()
            
            os.makedirs("data", exist_ok=True)
            
            if os.path.exists(messages_file):
                print(f"ğŸ“– æ­£åœ¨è¯»å–ç°æœ‰æ¶ˆæ¯æ–‡ä»¶...")
                try:
                    with open(messages_file, 'r', encoding='utf-8') as f:
                        existing_messages = json.load(f)
                    existing_dom_ids = {msg.get('domID') for msg in existing_messages}
                    print(f"âœ… è¯»å–åˆ° {len(existing_messages)} æ¡ç°æœ‰æ¶ˆæ¯\n")
                except Exception as e:
                    print(f"âš ï¸  è¯»å–ç°æœ‰æ¶ˆæ¯å¤±è´¥: {e}")
                    existing_messages = []
            else:
                print(f"â„¹ï¸  é¦–æ¬¡åˆ›å»ºæ¶ˆæ¯æ–‡ä»¶\n")
            
            # å»é‡ï¼šè¿‡æ»¤æ‰å·²å­˜åœ¨çš„ domID
            print(f"ğŸ” æ­£åœ¨æ£€æŸ¥é‡å¤æ¶ˆæ¯...")
            added_count = 0
            skipped_count = 0
            
            for msg in new_messages:
                dom_id = msg.get('domID')
                if dom_id not in existing_dom_ids:
                    existing_messages.append(msg)
                    existing_dom_ids.add(dom_id)
                    added_count += 1
                else:
                    skipped_count += 1
            
            print(f"âœ… æ–°å¢æ¶ˆæ¯: {added_count} æ¡")
            if skipped_count > 0:
                print(f"â­ï¸  è·³è¿‡é‡å¤: {skipped_count} æ¡")
            print()
            
            # æŒ‰æ—¶é—´æ’åº
            print("ğŸ“Š æ­£åœ¨æŒ‰æ—¶é—´æ’åº...")
            existing_messages.sort(key=lambda m: parse_timestamp(m.get('timestamp', '')))
            print(f"âœ… æ’åºå®Œæˆ\n")
            
            # å¯¼å‡ºä¸ºJSON
            print(f"ğŸ’¾ æ­£åœ¨å¯¼å‡ºåˆ°: {messages_file}")
            with open(messages_file, 'w', encoding='utf-8') as f:
                json.dump(existing_messages, f, ensure_ascii=False, indent=2)
            print(f"âœ… æ¶ˆæ¯å·²ä¿å­˜ (æ€»è®¡ {len(existing_messages)} æ¡)\n")
            
            # æ˜¾ç¤ºæ¶ˆæ¯ç»Ÿè®¡
            print("ğŸ“ˆ æ¶ˆæ¯ç»Ÿè®¡:")
            print(f"   - æœ¬æ¬¡æå–: {len(new_messages)}")
            print(f"   - æ–°å¢æ¶ˆæ¯: {added_count}")
            print(f"   - æ€»æ¶ˆæ¯æ•°: {len(existing_messages)}")
            
            # ç»Ÿè®¡ä½ç½®åˆ†å¸ƒ
            positions = {}
            for msg in existing_messages:
                pos = msg.get('position', 'unknown')
                positions[pos] = positions.get(pos, 0) + 1
            
            print(f"   - ä½ç½®åˆ†å¸ƒ:")
            for pos, count in positions.items():
                print(f"     â€¢ {pos}: {count}")
            
            # ç»Ÿè®¡å¼•ç”¨æ¶ˆæ¯æ•°é‡
            refer_count = sum(1 for msg in existing_messages if msg.get('refer'))
            print(f"   - åŒ…å«å¼•ç”¨: {refer_count}")
            
            # ç»Ÿè®¡åŒ…å«å†å²è®°å½•çš„æ¶ˆæ¯
            history_count = sum(1 for msg in existing_messages if msg.get('history'))
            print(f"   - åŒ…å«å†å²: {history_count}\n")
            
        except Exception as e:
            print(f"âŒ æ¶ˆæ¯æå–å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            print()
            messages_file = None

        print("\n" + "=" * 60)
        print("å¯¼å‡ºå®Œæˆï¼")
        print("=" * 60)
        print(f"\nğŸ“ è¾“å‡ºæ–‡ä»¶:")
        print(f"   1. HTML: {html_file}")
        print(f"   2. æˆªå›¾: {screenshot_file}")
        print(f"   3. åˆ†æ: {analysis_file}")
        if messages_file:
            print(f"   4. æ¶ˆæ¯: {messages_file} (å¢é‡æ›´æ–°)")
        print(f"\nğŸ’¡ ä¸‹ä¸€æ­¥:")
        print(f"   1. æ‰“å¼€ {html_file} æŸ¥çœ‹é¡µé¢ç»“æ„")
        print(f"   2. æŸ¥çœ‹ {screenshot_file} å¯¹ç…§å®é™…æ˜¾ç¤º")
        print(f"   3. é˜…è¯» {analysis_file} äº†è§£å¯ç”¨çš„é€‰æ‹©å™¨")
        if messages_file:
            print(f"   4. æŸ¥çœ‹ {messages_file} äº†è§£æå–çš„æ¶ˆæ¯å†…å®¹")
            print(f"   5. æ ¹æ®åˆ†æç»“æœè°ƒæ•´ message_extractor.py ä¸­çš„é€‰æ‹©å™¨")
        else:
            print(f"   4. æ ¹æ®åˆ†æç»“æœè°ƒæ•´ message_extractor.py ä¸­çš„é€‰æ‹©å™¨")
        print("=" * 60 + "\n")

    except Exception as e:
        print(f"\nâŒ å¯¼å‡ºå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

    finally:
        # å…³é—­æµè§ˆå™¨
        print("\nğŸ§¹ æ­£åœ¨æ¸…ç†èµ„æº...")
        await browser.close()
        print("âœ… æµè§ˆå™¨å·²å…³é—­")


if __name__ == "__main__":
    asyncio.run(export_page_dom())
